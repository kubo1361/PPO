{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72548,"status":"ok","timestamp":1635407018818,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"QgiyJ-hhu8gX","outputId":"beca9a80-53e6-46d3-ef1b-53950df6277e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'accept-rom-license'\u001b[0m\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.4.1)\n","Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.2.9)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.1.2.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[accept-rom-license,atari]) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[accept-rom-license,atari]) (0.16.0)\n","Collecting ALE\n","  Downloading Ale-0.8.4.tar.gz (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: ALE\n","  Building wheel for ALE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ALE: filename=Ale-0.8.4-py3-none-any.whl size=70179 sha256=6cac62fbe065cdf18d24bef07e5200af3ffa04a9d03f86ad948b260ebe36bddc\n","  Stored in directory: /root/.cache/pip/wheels/97/f8/c5/602c69f58d132fe0857de397228461b07b34829dc5102155bd\n","Successfully built ALE\n","Installing collected packages: ALE\n","Successfully installed ALE-0.8.4\n","/bin/bash: ale-import-roms: command not found\n","Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n","--2021-10-28 07:42:36--  http://www.atarimania.com/roms/Roms.rar\n","Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n","Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11128004 (11M) [application/x-rar-compressed]\n","Saving to: ‘Roms.rar’\n","\n","Roms.rar            100%[===================>]  10.61M  4.67MB/s    in 2.3s    \n","\n","2021-10-28 07:42:38 (4.67 MB/s) - ‘Roms.rar’ saved [11128004/11128004]\n","\n","\n","UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n","\n","\n","Extracting from /content/Roms.rar\n","\n","Extracting  /content/ROM/HC ROMS.zip                                     \b\b\b\b 36%\b\b\b\b\b  OK \n","Extracting  /content/ROM/ROMS.zip                                        \b\b\b\b 74%\b\b\b\b 99%\b\b\b\b\b  OK \n","All OK\n","copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n","copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n","copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n","copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n","copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n","copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n","copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n","copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n","copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n","copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n","copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n","copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n","copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n","copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n","copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n","copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n","copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n","copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n","copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n","copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n","copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n","copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n","copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n","copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n","copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n","copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n","copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n","copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n","copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n","copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n","copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n","copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n","copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n","copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n","copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n","copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n","copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n","copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n","copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n","copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n","copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n","copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n","copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n","copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n","copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n","copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n","copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n","copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n","copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n","copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n","copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n","copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n","copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n","copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n","copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n","copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n","copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n","copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n","copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n","copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n","copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n","copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n","copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n","copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n","copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n","copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n","copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n","copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n","copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n","copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n","copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n","copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n","copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n","copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n","copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n","copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n","Mounted at /content/gdrive\n"]}],"source":["# Notebook for training in google colab\n","!pip3 install gym[atari,accept-rom-license]\n","!pip3 install ALE\n","!ale-import-roms ALE\n","!pip3 install atari-py\n","\n","! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/\n","\n","import gym\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","import torch\n","import torch.optim as optim\n","import os\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=False)\n","DRIVE_PATH='/content/gdrive/MyDrive/Colab\\ Notebooks/ZHU/semestralka/'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1635407018821,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"LhKQqILRu8ga"},"outputs":[],"source":["# wrappers\n","class SkipEnv(gym.Wrapper):\n","    def __init__(self, env=None, skip=4):\n","        super(SkipEnv, self).__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        t_reward = 0.0\n","        done = False\n","\n","        for _ in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            t_reward += reward\n","            if done:\n","                break\n","        return obs, t_reward, done, info\n","\n","\n","class PreProcessFrame(gym.ObservationWrapper):\n","    def __init__(self, env=None):\n","        super(PreProcessFrame, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(\n","            low=0, high=255, shape=(80, 80, 1), dtype=np.uint8)\n","\n","    def observation(self, obs):\n","        return PreProcessFrame.process(obs)\n","\n","    @staticmethod\n","    def process(frame):\n","        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n","        new_frame = 0.299 * new_frame[:, :, 0] + 0.587 * \\\n","            new_frame[:, :, 1] + 0.114 * new_frame[:, :, 2]\n","\n","        new_frame = new_frame[0:160:2, ::2].reshape(80, 80, 1)\n","        return new_frame.astype(np.uint8)\n","\n","\n","class MoveImgChannel(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super(MoveImgChannel, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n","                                                shape=(self.observation_space.shape[-1],\n","                                                       self.observation_space.shape[0],\n","                                                       self.observation_space.shape[1]), dtype=np.float32)\n","\n","    def observation(self, observation):\n","        return np.moveaxis(observation, 2, 0)\n","\n","\n","class ScaleFrame(gym.ObservationWrapper):\n","    def observation(self, obs):\n","        return np.array(obs).astype(np.float32) / 255.0\n","\n","\n","class BufferWrapper(gym.ObservationWrapper):\n","    def __init__(self, env, n_steps):\n","        super(BufferWrapper, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(env.observation_space.low.repeat(n_steps, axis=0),\n","                                                env.observation_space.high.repeat(\n","                                                    n_steps, axis=0),\n","                                                dtype=np.float32)\n","\n","    def reset(self):\n","        self.buffer = np.zeros_like(\n","            self.observation_space.low, dtype=np.float32)\n","        return self.observation(self.env.reset())\n","\n","    def observation(self, observation):\n","        self.buffer[:-1] = self.buffer[1:]\n","        self.buffer[-1] = observation\n","        return self.buffer\n","\n","\n","def make_env(env_name, steps):\n","    env = gym.make(env_name)\n","    env = SkipEnv(env)\n","    env = PreProcessFrame(env)\n","    env = MoveImgChannel(env)\n","    env = BufferWrapper(env, steps)\n","    return ScaleFrame(env)\n","\n","\n","def transform_observation(obs):\n","    new = PreProcessFrame.process(obs)\n","    new = np.moveaxis(new, 2, 0)\n","    new = np.array(new).astype(np.float32) / 255.0\n","    return new"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1635407018823,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"PDKA0uZnu8gd"},"outputs":[],"source":["# custom reward\n","def reward_func(r):\n","    if r > 1:\n","        return 1\n","    elif r < -1:\n","        return -1\n","    return r\n","\n","# worker\n","class Worker:\n","    def __init__(self, id, env, agent, print_score=False, reward_function=reward_func):\n","        self.id = id\n","        self.env = env\n","\n","        self.print_score = print_score\n","        self.episode = 1\n","        self.steps = 0\n","        self.observation = None\n","        self.score = 0\n","        self.agent = agent\n","        self.reward_function = reward_function\n","\n","    def reset(self):\n","        if self.print_score and self.episode % 10 == 0:\n","            print('worker: ', self.id, '\\tepisode: ',\n","                  self.episode, '\\tsteps: ', self.steps, '\\tscore: ', self.score)\n","        self.agent.average_score.append(self.score)\n","        self.agent.average_steps.append(self.steps)\n","        self.agent.episodes += 1\n","        self.observation = self.env.reset()\n","        self.episode += 1\n","        self.score = 0\n","        self.steps = 0\n","        return self.observation\n","\n","    def step(self, action):\n","        self.observation, reward, terminate, _ = self.env.step(action)\n","        self.score += reward\n","        self.steps += 1\n","        reward = self.reward_function(reward)\n","\n","        return self.observation, reward, terminate\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1635407018826,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"ATsFYVteu8gf"},"outputs":[],"source":["def weights_init_xavier(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n","        init.xavier_uniform_(m.weight)\n","\n","# network\n","class network(nn.Module):\n","    def __init__(self, actions_count):\n","        super(network, self).__init__()\n","        self.actions_count = actions_count\n","\n","        self.conv1s = nn.Conv2d(4, 32, 3, stride=2, padding=1)  # B, CH, H, W\n","        self.attention_layer = MultiHeadAttention(32)\n","        self.conv2s = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n","        self.conv3s = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n","        self.conv4s = nn.Conv2d(64, 64, 3, stride=2, padding=1)\n","        self.conv5s = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n","        self.conv6s = nn.Conv2d(64, 32, 3, stride=1, padding=1)\n","\n","        self.fca1 = nn.Linear(5 * 5 * 32, 512)\n","        self.fcc1 = nn.Linear(5 * 5 * 32, 512)\n","\n","        self.fca2 = nn.Linear(512, actions_count)\n","        self.fcc2 = nn.Linear(512, 1)\n","\n","        self.apply(weights_init_xavier)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1s(x))\n","\n","        x = self.attention_layer(x, x, x)\n","        \n","        x = F.relu(self.conv2s(x))\n","        x = F.relu(self.conv3s(x))\n","        x = F.relu(self.conv4s(x))\n","        x = F.relu(self.conv5s(x))\n","        x = F.relu(self.conv6s(x))\n","\n","        x = x.flatten(start_dim=1)\n","\n","        x_a = F.relu(self.fca1(x))\n","        x_c = F.relu(self.fcc1(x))\n","\n","        outActor = self.fca2(x_a)\n","        outCritic = self.fcc2(x_c)\n","\n","        action = F.softmax(outActor, dim=-1).detach()\n","\n","        action = action.multinomial(num_samples=1)\n","\n","        return outActor, outCritic, action\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, size):\n","        super().__init__()\n","        self.w_qs = nn.Conv2d(size, size, 1)\n","        self.w_ks = nn.Conv2d(size, size, 1)\n","        self.w_vs = nn.Conv2d(size, size, 1)\n","\n","\n","    def forward(self, q, k, v):\n","        residual = q\n","\n","        q = self.w_qs(q).permute(0, 2, 3, 1)\n","        k = self.w_ks(k).permute(0, 2, 3, 1)\n","        v = self.w_vs(v).permute(0, 2, 3, 1)\n","\n","        attn = torch.matmul(q, k.transpose(2, 3))\n","\n","        attention = torch.matmul(attn, v).permute(0, 3, 1, 2)\n","\n","        out = attention + residual\n","        return out\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1635407018829,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"K-skaD91wBSC"},"outputs":[],"source":["# download from drive\n","def download_model(name, id, suffix):\n","    path = '{name}/{name}_{id}_{suffix}.pt'.format(name=name,id=id, suffix=suffix)\n","    path_from = DRIVE_PATH + path\n","    path_to = '/content/models/{name}'.format(name=name)\n","    \n","    if not os.path.exists(path_to):\n","        os.makedirs(path_to)\n","\n","    !cp {path_from} {path_to} \n","    model = path_to + '/{name}_{id}_{suffix}.pt'.format(name=name,id=id, suffix=suffix)\n","    return model\n","\n","# upload to drive\n","def upload_model():\n","    model = '/content/models/*'\n","    stats = '/content/runs/*'\n","\n","    !cp -r {model} {DRIVE_PATH}\n","    !cp -r {stats} {DRIVE_PATH}\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":844,"status":"ok","timestamp":1635411164457,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"7fNCYbTcu8gg"},"outputs":[],"source":["class AgentPPO:\n","    def __init__(self, name, model, \n","                 gamma=0.99, lr=0.0001, beta_entropy=0.01, critic_loss_coef=0.5, \n","                 grad_clip=0.1, epsilon=0.2, lr_decay=1e-7, id=0):\n","        # init vars\n","        self.model = model\n","        self.gamma = gamma\n","        self.beta_entropy = beta_entropy\n","        self.critic_loss_coef = critic_loss_coef\n","        self.grad_clip = grad_clip\n","        self.epsilon = epsilon\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","\n","        # device - define and cast\n","        self.device = torch.device(\n","            \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        print('Device: ', self.device)\n","        self.model.to(self.device)\n","\n","        # define optimizer\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n","\n","        # create vars for tracking progress and identification\n","        # used by workers\n","        self.average_score = []\n","        self.average_steps = []\n","        self.episodes = 0\n","\n","        # for convenience sake\n","        self.best_avg = 0\n","\n","        # identification\n","        self.name = name\n","        self.id = id\n","\n","        # create folders for models and logs\n","        self.writer = SummaryWriter('runs/' + self.name + '/' + str(self.id))\n","        self.model_path = 'models/' + self.name + '/'\n","\n","        if not os.path.exists(self.model_path):\n","            os.makedirs(self.model_path)\n","\n","\n","\n","    def load_model(self, path):\n","        self.model.load_state_dict(torch.load(path,map_location=torch.device(self.device)))\n","\n","\n","    def save_model(self, path):\n","        torch.save(self.model.state_dict(), path)\n","\n","\n","    def train(self, workers, episodes, steps, epochs=4, observations_per_epoch=4, \n","              start_episode=0, start_score=0, start_steps=0):\n","        self.model.train()\n","        \n","        # initial variables\n","        self.average_score = [start_score]\n","        self.average_steps = [start_steps]\n","        new_observations = []\n","        self.best_avg = start_score + 100\n","        self.episodes = start_episode \n","        iteration = 0\n","        len_workers = len(workers)\n","\n","        all_epochs_per_iteration = steps * epochs\n","        index_range = torch.arange(all_epochs_per_iteration)\n","\n","        # initial observations\n","        for worker in workers:\n","            new_observations.append(torch.from_numpy(worker.reset()).float())\n","        new_observations = torch.stack(new_observations).to(self.device)\n","\n","        self.writer.add_graph(self.model, new_observations)\n","\n","        while(True):\n","            iteration = iteration + 1\n","            iter_critic_values = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            iter_actor_log_probs = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            iter_actions = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            iter_rewards = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            iter_not_terminated = torch.ones([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            raw_advantages = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            diff_advantages = torch.zeros([all_epochs_per_iteration, len_workers, 1]).to(self.device)\n","            old_observations = []\n","\n","\n","            for epoch in range(all_epochs_per_iteration):\n","                # first forward pass with fresh observation\n","                with torch.no_grad():\n","                    epoch_actor, epoch_critic, epoch_actor_actions = self.model(new_observations)\n","\n","                # after use it becomes old\n","                old_observations.append(new_observations)\n","                new_observations = []\n","\n","                # epoch logarithmic probabilities\n","                epoch_log_probs = F.log_softmax(epoch_actor, dim=-1)\n","                epoch_log_policy = epoch_log_probs.gather(1, epoch_actor_actions)\n","\n","                # reset epoch specific variables\n","                epoch_rewards = torch.zeros([len_workers, 1])\n","                epoch_not_terminated = torch.ones(\n","                    [len_workers, 1], dtype=torch.int8)\n","\n","                # generate new observations for next pass and rewards for actual actions\n","                for worker in range(len_workers):\n","                    # Apply actions to workers enviroments\n","                    worker_observation, epoch_rewards[worker, 0], worker_terminated = workers[worker].step(\n","                        epoch_actor_actions[worker].item())\n","\n","                    # reset terminated workers\n","                    if worker_terminated:\n","                        epoch_not_terminated[worker, 0] = 0\n","                        worker_observation = workers[worker].reset()\n","\n","                    # append new observations\n","                    new_observations.append(torch.from_numpy(\n","                        worker_observation).float())\n","\n","                # update iteration specific variables\n","                iter_critic_values[epoch] = epoch_critic\n","                iter_actor_log_probs[epoch] = epoch_log_policy\n","                iter_actions[epoch] = epoch_actor_actions\n","                iter_rewards[epoch] = epoch_rewards\n","                iter_not_terminated[epoch] = epoch_not_terminated\n","                new_observations = torch.stack(new_observations).to(self.device)\n","\n","\n","            # second forward pass for critic values on new observations\n","            with torch.no_grad():\n","                _, new_critic_values, _ = self.model(new_observations)\n","\n","            # compute advantage - we compute advantage backwards through all epochs\n","            # with their respective critic values for each epoch\n","            for epoch in reversed(range(all_epochs_per_iteration)):\n","                new_critic_values = iter_rewards[epoch] + \\\n","                    (self.gamma * new_critic_values * iter_not_terminated[epoch])\n","\n","                diff_advantages[epoch] = new_critic_values - iter_critic_values[epoch]\n","                raw_advantages[epoch] = new_critic_values \n","\n","            # standard score normalization of advantage\n","            raw_advantages = (raw_advantages - torch.mean(raw_advantages)) / \\\n","                (torch.std(raw_advantages) + 1e-5)\n","            \n","            diff_advantages = (diff_advantages - torch.mean(diff_advantages)) / \\\n","                (torch.std(diff_advantages) + 1e-5)\n","\n","            for epoch in range(epochs):\n","                index = index_range[torch.randperm(all_epochs_per_iteration)].flatten(start_dim=0)\n","                for batch in range(observations_per_epoch):\n","                    epoch_index = index[epoch]\n","                    epoch_observation = old_observations[epoch_index]\n","\n","                    epoch_actor, epoch_critic, _ = self.model(epoch_observation)\n","                    \n","                    new_epoch_log_probs = F.log_softmax(epoch_actor, dim=-1)\n","\n","                    new_epoch_log_policy = new_epoch_log_probs.gather(1, iter_actions[epoch_index].type(torch.int64))     \n","                    \n","                    epoch_probs = F.softmax(epoch_actor, dim=-1)\n","                    epoch_entropies = (\n","                        new_epoch_log_probs * epoch_probs).sum(1, keepdim=True)\n","\n","                    new_advantage = raw_advantages[epoch_index] - epoch_critic\n","\n","                    ratio = torch.exp(new_epoch_log_policy - iter_actor_log_probs[epoch_index])\n","                    epoch_advangate = diff_advantages[epoch_index]\n","                    clip = torch.clamp(ratio, min=1 - self.epsilon, max=1 + self.epsilon)\n","\n","                    ratio_adv = ratio * epoch_advangate\n","                    clip_adv = clip * epoch_advangate\n","\n","                    actor_loss = - torch.min(ratio_adv, clip_adv).mean()\n","                    # actor_loss = - (iter_actor_log_probs * advantages_detached).mean()\n","                    critic_loss = (new_advantage**2).mean() * self.critic_loss_coef\n","                    entropy_loss = (epoch_entropies.mean() * self.beta_entropy)\n","\n","                    # print(\"actor_loss: \", actor_loss.item(), \"critic_loss: \", critic_loss.item(), \"entropy_loss: \", entropy_loss.item())\n","            \n","                    # clear gradients\n","                    self.optimizer.zero_grad()\n","\n","                    # calculate final loss\n","                    loss = actor_loss + critic_loss + entropy_loss\n","\n","                    # backward pass with our total loss https://www.datahubbs.com/two-headed-a2c-network-in-pytorch/\n","                    loss.backward()\n","\n","                    # gradient clipping for exploding gradients https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n","\n","                    # optimizer step\n","                    self.optimizer.step()\n","\n","            self._write(iteration, actor_loss, critic_loss, entropy_loss, loss)\n","\n","            if (episodes == self.episodes):\n","                self.writer.close()\n","                return   \n","        \n","\n","\n","    def _write(self, iteration, actor_loss, critic_loss, entropy_loss, loss):\n","         # stats\n","        if iteration % 10 == 0 and iteration > 0:\n","            # average for last 100 scores\n","            avg_score = np.average(self.average_score[-100:])\n","            avg_steps = np.average(self.average_steps[-100:])\n","\n","            # save model on new best average score\n","            if avg_score > self.best_avg:\n","                self.best_avg = avg_score\n","                print('Best model save, episode: ', self.episodes, ' score: ',\n","                      self.best_avg)\n","                model_filename = (\n","                    self.model_path + self.name + '_' + str(self.id) + '_best.pt')\n","                self.save_model(model_filename)\n","                upload_model()\n","\n","        if iteration % 50 == 0 and iteration > 0:\n","            # lower learning rate\n","            self.lr = max(self.lr - self.lr_decay, 1e-7)\n","            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n","\n","            # display informations\n","            print('episodes: ',\n","                    self.episodes, '\\taverage steps: ', avg_steps, '\\taverage score: ', avg_score)\n","\n","            # write to tensorboard\n","            self.writer.add_scalar('Actor loss',\n","                    actor_loss.item(), self.episodes)\n","\n","            self.writer.add_scalar('Critic loss',\n","                    critic_loss.item(), self.episodes)\n","\n","            self.writer.add_scalar('Entropy loss',\n","                    entropy_loss.item(), self.episodes)\n","\n","            self.writer.add_scalar('Final loss',\n","                    loss.item(), self.episodes)\n","\n","            self.writer.add_scalar('Average steps per 100 episodes',\n","                    avg_steps, self.episodes)\n","\n","            self.writer.add_scalar('Average score per 100 episodes',\n","                    avg_score, self.episodes)\n","\n","        if iteration % 500 == 0 and iteration > 0:\n","            self.average_score = self.average_score[-100:]\n","            self.average_steps = self.average_steps[-100:]\n","            continuous_save_model_filename = (\n","                self.model_path + self.name + '_' + str(self.id) + '_' + str(self.episodes) + '.pt')\n","            self.save_model(continuous_save_model_filename)\n","\n","            print('Periodic model save, episode: ', self.episodes)\n","            upload_model()\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10764,"status":"ok","timestamp":1635407806756,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"I4KjDsMxu8gh","outputId":"4a5685a6-ec7e-416b-d085-0c5a70bee0db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cpu\n"]}],"source":["# train\n","actions = 5\n","workers_len = 25\n","stack = 4\n","steps = 5\n","epochs = 5\n","observations_per_epoch = 5\n","name = \"test_pacman\"\n","id = 0\n","\n","goal_episodes = 500000\n","start_episode=11677\n","start_steps=200\n","start_score=760\n","\n","agent = AgentPPO(name=name, model=network(actions), id=id)\n","\n","workers = []\n","for id_w in range(workers_len):\n","    env = make_env('MsPacman-v0', stack)\n","    env.seed(id_w)\n","    w = Worker(id_w, env, agent, print_score=False)\n","    workers.append(w)\n","\n","agent.load_model(download_model(name,id, 'last'))"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":716},"executionInfo":{"elapsed":3270527,"status":"error","timestamp":1635414457322,"user":{"displayName":"jakub dzurovcin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137092399323810675"},"user_tz":-120},"id":"tPdav8lt5n1_","outputId":"7f09f027-e167-4890-d18b-3e501c8acbca"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/jit/_trace.py:985: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n","\t%296 : Long(25, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::multinomial(%action, %293, %294, %295) # <ipython-input-4-aa3bbcb6404d>:49:0\n","This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n","  _module_class,\n","/usr/local/lib/python3.7/dist-packages/torch/jit/_trace.py:985: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n","With rtol=1e-05 and atol=0, found 16 element(s) (out of 25) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 4.0 (4.0 vs. 0.0), which occurred at index (5, 0).\n","  _module_class,\n"]},{"name":"stdout","output_type":"stream","text":["episodes:  11853 \taverage steps:  187.04 \taverage score:  682.2\n","episodes:  12004 \taverage steps:  204.2 \taverage score:  733.7\n","episodes:  12168 \taverage steps:  191.0 \taverage score:  787.0\n","episodes:  12335 \taverage steps:  185.83 \taverage score:  758.0\n","Best model save, episode:  12467  score:  876.2\n","episodes:  12493 \taverage steps:  201.28 \taverage score:  870.9\n","episodes:  12658 \taverage steps:  187.15 \taverage score:  668.4\n","episodes:  12819 \taverage steps:  198.0 \taverage score:  795.7\n","episodes:  12986 \taverage steps:  191.84 \taverage score:  847.6\n","Best model save, episode:  13101  score:  954.6\n","episodes:  13136 \taverage steps:  208.13 \taverage score:  866.2\n","episodes:  13286 \taverage steps:  206.77 \taverage score:  771.2\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a2b73a3c252a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservations_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mstart_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             start_steps=start_steps)\n\u001b[0m","\u001b[0;32m<ipython-input-12-01cf63cf715a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, workers, episodes, steps, epochs, observations_per_epoch, start_episode, start_score, start_steps)\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-01cf63cf715a>\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, iteration, actor_loss, critic_loss, entropy_loss, loss)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinuous_save_model_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Periodic model save, episode: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mupload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'AgentPPO' object has no attribute 'episode'"]}],"source":["agent.train(workers=workers, episodes=goal_episodes, steps=steps, \n","            epochs=epochs, observations_per_epoch=observations_per_epoch, \n","            start_episode=start_episode, start_score=start_score, \n","            start_steps=start_steps)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"colab_train.ipynb","provenance":[]},"interpreter":{"hash":"9a5bf0e192955ffeab1f156d06071d249074d0636da325ecbc89967f16fe4b5e"},"kernelspec":{"display_name":"Python 3.9.7 64-bit (windows store)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
